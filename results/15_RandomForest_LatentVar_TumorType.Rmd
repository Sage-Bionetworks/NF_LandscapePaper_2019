---
title: "RF_latent_variable"
author: "Jineta Banerjee"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: 
  html_document:
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    fig_width: 7
    fig_height: 6
    fig_caption: true
    df_print: paged
    code_folding: hide
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, eval=TRUE, results='hide', message=FALSE, warning=FALSE, include=FALSE}

library(synapser)
library(synapserutils)
library(BiocManager)

library(tidyverse)
library(DT)
library(colorspace)
library(RColorBrewer)
library(wesanderson)

#Random Forest
library(randomForest)
library(e1071)
library(caret)

#plotting
library(AppliedPredictiveModeling)
transparentTheme(trans = .4)
library(pheatmap)

library(AnnotationDbi)
#library(hgu95av2.db)
#library(STRINGdb)
library(gridExtra)

#renv::init()
library(glue)

#Synapser
synLogin()
```


```{r make color palette}

## Make colors for plots
fabcolors = RColorBrewer::brewer.pal(n = 11,name = 'RdGy')
col1 = RColorBrewer::brewer.pal(n = 10,name = 'PRGn')
col2 = RColorBrewer::brewer.pal(n = 10,name = 'Spectral')
col3 = RColorBrewer::brewer.pal(n = 10,name = 'BrBG')
col4 = RColorBrewer::brewer.pal(n = 10,name = 'PiYG')
col5 = RColorBrewer::brewer.pal(n = 10,name = 'PuOr')


allcolors <- c(fabcolors, col1,col2,col3, col4, col5)
allcolors <- list(allcolors)

morecolors1 <- wes_palette("Darjeeling1", n=4, type = "discrete")
morecolors1 <- list(morecolors1)

morecolors2 <- wes_palette("Moonrise2", n=3, type = "discrete")
morecolors2 <- list(morecolors2)

color_list <- c(allcolors, morecolors1, morecolors2)

```

## Introduction

This document describes training a random forest model using latent variables generated by [transfer learning approaches](https://www.cell.com/cell-systems/pdfExtended/S2405-4712(19)30119-X) as its input features. We are using the latent variables found in NF1 tumor data to train the forest to find the most important classifying LVs for the four tumor types in NF1. We chose to train a random forest classifier to identify NF tumortypes using the LVs for the following features of the model :

* robustness to high dimensionality data
* ability to handle unbalanced classes
* robustness to outliers and non-linear data
* quick training /prediction speeds
* low bias and moderate variance

Our goal is to find important latent variables that classify the various tumorTypes. We will then inspect the classifying features to find meaningful genesets that distinguish between two tumortypes.

Tumortypes represented in the data:

* Plexiform Neurofibroma
* MPNST
* Cutaneous Neurofibroma
* Neurofibroma

```{r download data from Synapse, eval=TRUE, results='hide', message=FALSE, warning=FALSE, include=FALSE}

#download data: LV by gene loadings
lv_entity <- synGet("syn18689545")
lv_all_pn <- readRDS(lv_entity$path)
gene_by_lv <- as.data.frame(lv_all_pn$Z)
recount_sample_by_lv <- as.data.frame(lv_all_pn$B)

# Load LVs of NF1 samples except the xenografts
lv_by_NF <- synTableQuery("SELECT * FROM syn21046991")$asDataFrame() %>%
  filter(tumorType!="NA",
         !grepl('xenograft', specimenID, ignore.case = T),
         !specimenID %in% c("BI386-004","CW225-001","DW356-002",
                            "JK368-003", "SK436-005"))

keep <- c("latent_var", "value", "specimenID")
lv_by_NF_select <- lv_by_NF[ ,keep] %>%
  group_by_at(vars(-value)) %>%  # group by everything other than the value column.
  mutate(row_id=1:n()) %>% ungroup() %>%  # build group index
  spread(key=latent_var, value=value) %>%    # spread
  dplyr::select(-row_id)   # drop the index 

specimen_tumortype <- unique(lv_by_NF[,c("specimenID", "tumorType")])

forest_data <- merge(lv_by_NF_select, specimen_tumortype, by = "specimenID")
rownames(forest_data) <- forest_data$specimenID
forest_data$tumorType <- as.factor(forest_data$tumorType)
forest_data <- forest_data[,2:ncol(forest_data)]

```


## Partitioning the data into training and testing set:

The dataset was split into 75% training and 25% testing dataset. The function _createDataPartition_ is used to create balanced splits of the data. Since the _tumorType_ argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data.

```{r split the data for training, eval=TRUE}

#Make the test and training datasets
set.seed(998)
inTraining <- createDataPartition(as.factor(forest_data$tumorType), p = .75, list = FALSE)

training <- forest_data[ inTraining,]
testing  <- forest_data[-inTraining,]

```

## Model training and Crossvalidation :

We first trained an initial model on the training dataset using iterative _mtrys_ and 500 trees. The model was crossvalidated using 10-fold crossvalidation technique.

Then we tuned the model parameters to iterate through 1:100 different features to split the trees on (mtrys). We also increased the number of trees to 1000 to increase its accuracy. To account for adequate sample size for each validation round, 5-fold crossvalidation was carried out. Below are details of our tuned model.

```{r create model and check fit, eval=TRUE, fig.height=10, fig.width=10}


## Load Fit data (The Model described in this document is stored on Synapse)
load(synGet("syn21201190")$path)

# 10 fold validation control
fitControl <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 5)

tunegrid <- expand.grid(.mtry=c(1:100))

#Find the classes:
summary(training$tumorType)


## Construct the random forest model called Fit (the code is commented out to facilitate quick rendering of html file by loading the Fit from Synapse)

# set.seed(9998)
# Fit <- train(tumorType ~ .,
#              data = training[,c(1:ncol(training))],
#              method = "rf",
#              ntree= 1000,
#              tuneGrid = tunegrid,
#              #classwt =
#              proximity=TRUE,
#              importance = TRUE,
#              trControl = fitControl,
#              verbose = TRUE)

print(" Check the fit of the model")                 
Fit

#plot the model
theme_update(text = element_text(size=20))
ggplot(Fit$results,  aes(x=mtry, y=Accuracy)) +
  geom_point(aes(size=2)) +
  geom_line() +
  coord_cartesian(ylim = c(0,1)) +
  labs(main="The model", x="mtry :: Number of features for each split", y= "Accuracy of the model") 

print("Check the clustering of the samples according to the model")
MDSplot(Fit$finalModel, 
        as.factor(training$tumorType), 
        k=2, palette=NULL, 
        pch=as.numeric(training$tumorType), 
        cex=2, 
        cex.axis= 1.5,
        cex.lab = 1.5,
        cex.main = 1.5,
        main= "MDS Plot of the classified training set")
legend("topright",
       inset=0.01, 
       cex= 1.2,
       legend=levels(training$tumorType), 
       fill=brewer.pal(6, "Set1"))

#Order errors using OOB
#head(Fit$finalModel$err.rate[order(Fit$finalModel$err.rate[,1]),])

```

## Predict the test set with our tuned model
```{r predict model, eval=TRUE}

#Use model to predict labels of test data
pred <- predict(Fit, newdata = testing[,c(1:length(colnames(testing)))])

#store predicted labels in the test dataframe
testing$pred <- pred

```

### Check the quality of prediction

The confusion matrix of the prediction using the model is given below.
```{r model accuracy, eval=TRUE}

# Check the accuracy of the model
library(DT)

conf_matrix <- confusionMatrix(data = testing$pred, 
                              reference = as.factor(testing$tumorType), 
                              mode = "prec_recall")

conf_matrix

#conf_matrix$table

```

Even though our model accuracy on the training data seemed to stagnate around 77%, a close look at the confusion matrix of the test set shows high F1 scores for all our test classes. So we decided to go ahead with this model and inspect the features it deemed important for the classification.

## Important variables for the model

Lets take a look at the important latent variables picked up by our model as classifiers for the different classes.

```{r visualize predictor feature set, eval=TRUE, fig.width=10, fig.height=10}

# estimate variable importance
importance <- varImp(Fit, scale=FALSE)

# Plot the importance of the variables
varImpPlot(Fit$finalModel,
           main = "Important variables in the forest",
           n.var = 20)


# Select top important features
list <- as.data.frame(importance$importance)

## Transform list to enable plotting
list$LV <- rownames(list)
list$LV <- gsub("`", "", list$LV)
##Function to extract numbers from data in columns
library(stringr)
numextract <- function(string){ 
  str_extract(string, "\\-*\\d+\\.*\\d*")
} 
list$LatentVar <- numextract(list$LV)
list$LatentVar <- glue('V{list$LatentVar}')


#Find important vars for each class
MPNST_list <- list[order(-list$`Malignant Peripheral Nerve Sheath Tumor`), ]
pNF_list <- list[order(-list$`Plexiform Neurofibroma`), ]
cNF_list <- list[order(-list$`Cutaneous Neurofibroma`), ]
NF_list <- list[order(-list$`Neurofibroma`), ]


# Plot top features for cNF

featureHeatmap <- function(forest_data, list, numfeatures, list_title){
  "
  This function plots a heatmap of all the important features deemed as classifiers of a class
  
  Expected Inputs:
  1. forest_data <- dataframe used to generate the model (with numeric values of all features for all the classes, specimenID as rownames, column names tumorType)
  2. list <- list of important variables for a specific class
  3. numfeatures <- number of features to be plotted
  4. list_title <- name of the list to be plotted as heatmap
  
  Expected Output:
  pheatmap (tumortype by features)
  "
  keep <- c(colnames(forest_data) %in% list$LV[1:numfeatures])
  plot_data <- forest_data[ , keep]
  plot_data$tumorType <- forest_data$tumorType
  plot_data$specimenID <- rownames(plot_data)
  pheatmap(plot_data[,1:numfeatures],
                       show_rownames = F,
                       labels_col = colnames(plot_data)[1:numfeatures],
                       #fontsize_row = 0,
                       clustering_method = 'ward.D2',
                       cluster_rows = T,
                       scale = "column",
                       #annotation_col = colnames(plot_data),
                      #annotation_colors = c(color_list[[1]],color_list[[2]],color_list[[3]]),
                       annotation_row= plot_data[,c("specimenID", "tumorType")],
                       width = 8, 
                       height = 8,
                       main= glue('Heatmap of features from {list_title}'))
}

featureHeatmap(forest_data, cNF_list, 50, "Cutaneous Neurofibroma")
featureHeatmap(forest_data, NF_list, 50, "Neurofibroma")
featureHeatmap(forest_data, MPNST_list, 50, "MPNST")
featureHeatmap(forest_data, pNF_list, 50, "Plexiform Neurofibroma")

```


## Plot the gene loadings the top 5 important LVs for each of the tumortypes

### Cutaneous Neurofibroma

```{r Plot cNF loadings, eval=T, fig.width=10, fig.height=10}

## Plot the genes in the top LVs

plotGenes <- function(plot_data, list_var){
  
  for (i in (unique(list_var$LatentVar)[1:5])) {
 
  tidy <- plot_data %>%
    dplyr::select(i) %>% 
    tibble::rownames_to_column('genes')
  
  p <- ggplot(tidy %>% top_n(30, get(i))) +
    aes(x=reorder(genes, -get(i)), y=get(i)) +
    geom_bar(stat="identity") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    ylim(c(0,8)) +
    xlab("Genes") +
    ylab("Loadings") +
    ggtitle(glue('Gene Loadings for interesting LV: {i}'))
  
  print(p)
  
  }

}

plotGenes(gene_by_lv, cNF_list)


```

### Neurofibroma

```{r Plot NF loadings, eval=T, fig.width=10, fig.height=10}

## Plot the genes in the top LVs

plotGenes(gene_by_lv, NF_list)

```

### Plexiform Neurofibroma

```{r Plot pNF loadings, eval=T, fig.width=10, fig.height=10}

## Plot the genes in the top LVs

plotGenes(gene_by_lv, pNF_list)


```

### MPNST

```{r plot MPNST loadings, eval=T, fig.width=10, fig.height=10}

## Plot the genes in the top LVs

plotGenes(gene_by_lv, MPNST_list)

```

```{r save imp datafiles, eval=F, fig.width=10, fig.height=10}

save(Fit, forest_data, file = "RandomForest_LatentVar_Tumortypes.Rdata")
```

```{r sessionInfo, eval=T}

sessionInfo()

```
